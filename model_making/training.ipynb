{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf696f89",
   "metadata": {},
   "source": [
    "# Imitation learning training script!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "860adac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pathlib\n",
    "\n",
    "import random\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af541de",
   "metadata": {},
   "source": [
    "## Check if CUDA is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf0e7198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc68ddf",
   "metadata": {},
   "source": [
    "## Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d21a86fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_x = 2.5\n",
    "max_z = 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "322f0c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from all sessions...\n",
      "Total samples loaded: 27710\n",
      "Training samples: 22168\n",
      "Validation samples: 5542\n"
     ]
    }
   ],
   "source": [
    "# Load all sessions and combine into a single pool\n",
    "base_path = pathlib.Path(\"/home/kazuh/Documents/UBC/ENPH_353/training_data/all_sessions\")\n",
    "session_dirs = sorted([d for d in base_path.iterdir() if d.is_dir()])\n",
    "\n",
    "all_img_paths = []\n",
    "all_x_vals = []\n",
    "all_z_vals = []\n",
    "\n",
    "print(\"Loading data from all sessions...\")\n",
    "for session_dir in session_dirs:\n",
    "    labels_path = session_dir / \"labels.csv\"\n",
    "    df = pd.read_csv(labels_path, header=None, skiprows=1)\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        img_path = session_dir / row[0]\n",
    "        all_img_paths.append(img_path)\n",
    "        all_x_vals.append(row[1] / max_x)\n",
    "        all_z_vals.append(row[2] / max_z)\n",
    "\n",
    "print(f\"Total samples loaded: {len(all_img_paths)}\")\n",
    "\n",
    "# Convert to numpy arrays\n",
    "all_x_vals = np.array(all_x_vals)\n",
    "all_z_vals = np.array(all_z_vals)\n",
    "\n",
    "# Randomize the order\n",
    "indices = np.arange(len(all_img_paths))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "all_img_paths = [all_img_paths[i] for i in indices]\n",
    "all_x_vals = all_x_vals[indices]\n",
    "all_z_vals = all_z_vals[indices]\n",
    "\n",
    "# Split into train and validation sets (80/20 split)\n",
    "split_idx = int(0.8 * len(all_img_paths))\n",
    "\n",
    "train_img_paths = all_img_paths[:split_idx]\n",
    "train_x_vals = all_x_vals[:split_idx]\n",
    "train_z_vals = all_z_vals[:split_idx]\n",
    "\n",
    "val_img_paths = all_img_paths[split_idx:]\n",
    "val_x_vals = all_x_vals[split_idx:]\n",
    "val_z_vals = all_z_vals[split_idx:]\n",
    "\n",
    "print(f\"Training samples: {len(train_img_paths)}\")\n",
    "print(f\"Validation samples: {len(val_img_paths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "621485ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DrivingDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, img_paths, x_vals, z_vals):\n",
    "        self.img_paths = img_paths\n",
    "        self.x_vals = x_vals\n",
    "        self.z_vals = z_vals\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = str(self.img_paths[idx])\n",
    "\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            raise RuntimeError(f\"Could not load image: {img_path}\")\n",
    "\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = img.astype(np.float32) / 255.0\n",
    "        img = np.transpose(img, (2, 0, 1))  # (H,W,C)->(C,H,W)\n",
    "\n",
    "        x = np.array([self.x_vals[idx], self.z_vals[idx]], dtype=np.float32)\n",
    "\n",
    "        return torch.tensor(img), torch.tensor(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "034df3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dataset = DrivingDataset(train_img_paths, train_x_vals, train_z_vals)\n",
    "val_dataset   = DrivingDataset(val_img_paths, val_x_vals, val_z_vals)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6857221",
   "metadata": {},
   "source": [
    "## Design Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05c5c382",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JackDrivingModel(nn.Module):\n",
    "\n",
    "    def __init__(self, img_channels = 3, img_h = 600, img_w = 800, act_dim = 2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(img_channels, 24, kernel_size=5, stride=2), nn.ReLU(),\n",
    "            nn.Conv2d(24, 36, kernel_size=5, stride=2), nn.ReLU(),\n",
    "            nn.Conv2d(36, 48, kernel_size=5, stride=2), nn.ReLU(),\n",
    "            nn.Conv2d(48, 64, kernel_size=3, stride=1), nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1), nn.ReLU()\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, img_channels, img_h, img_w)\n",
    "            conv_out = self.conv(dummy)\n",
    "            conv_feat_dim = conv_out.view(1, -1).shape[1]\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_feat_dim, 100), nn.ReLU(),\n",
    "            nn.Linear(100, 50), nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(50, act_dim)  # [vx, wz]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94d6d0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_model_weights(model):\n",
    "    \"\"\"\n",
    "    Reinitializes all model parameters by calling reset_parameters()\n",
    "    for each layer that implements it (Conv, Linear, etc.).\n",
    "    \"\"\"\n",
    "    for layer in model.modules():\n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "            layer.reset_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d081eec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def omega_more_bad(preds, targets):\n",
    "    diff = torch.abs(preds - targets)\n",
    "    weights = torch.tensor([1.0, 2.0], dtype=preds.dtype, device=preds.device)\n",
    "    return (diff * weights).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454d60be",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ff8a6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = JackDrivingModel(img_channels=3, img_h=600, img_w=800, act_dim=2)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = omega_more_bad\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8b49ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_model_weights(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd3e4eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for imgs, targets in loader:\n",
    "        imgs = imgs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "    return running_loss / len(loader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83ef6d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, targets in loader:\n",
    "            imgs = imgs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            running_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "    return running_loss / len(loader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8bd9019",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs):\n",
    "\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "\n",
    "    scheduler = ReduceLROnPlateau(optimizer=optimizer, mode = \"min\", patience=5, factor=0.5)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        t0 = time.time()\n",
    "\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss = validate(model, val_loader, criterion, device)\n",
    "\n",
    "        train_loss_history.append(train_loss)\n",
    "        val_loss_history.append(val_loss)\n",
    "    \n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} \"\n",
    "              f\"| Time: {time.time() - t0:.1f}s \"\n",
    "              f\"| Train Loss: {train_loss:.4f} \"\n",
    "              f\"| Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "            \n",
    "    plt.plot(train_loss_history, label=\"Train Loss\")\n",
    "    plt.plot(val_loss_history, label=\"Val Loss\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44c02a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m45\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(num_epochs)\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[32m     11\u001b[39m     t0 = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     train_loss = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m     val_loss = validate(model, val_loader, criterion, device)\n\u001b[32m     16\u001b[39m     train_loss_history.append(train_loss)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, loader, optimizer, criterion, device)\u001b[39m\n\u001b[32m     13\u001b[39m     loss.backward()\n\u001b[32m     14\u001b[39m     optimizer.step()\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     running_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m * imgs.size(\u001b[32m0\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m running_loss / \u001b[38;5;28mlen\u001b[39m(loader.dataset)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    train(15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b37008",
   "metadata": {},
   "source": [
    "# Convert to PT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20839e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_export = JackDrivingModel(img_channels=3, img_h=600, img_w=800, act_dim=2)\n",
    "\n",
    "state = torch.load(\"/home/kazuh/Documents/UBC/ENPH_353/training_data/model_making/11_29_morning.pth\")\n",
    "model_export.load_state_dict(state)\n",
    "\n",
    "model_export.eval()\n",
    "scripted_model = torch.jit.script(model_export)\n",
    "\n",
    "scripted_model.save(\"11_29_morning.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JB_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
